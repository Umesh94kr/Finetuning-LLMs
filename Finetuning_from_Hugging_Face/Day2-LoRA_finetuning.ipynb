{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd0a265e",
   "metadata": {},
   "source": [
    "#### **What is LoRA?**\n",
    "\n",
    "LoRA is a technique that fine-tunes large language models by freezing the original weights and learning small low-rank matrices instead. This makes training faster, cheaper, and memory-efficient.\n",
    "\n",
    "LoRA solves this by freezing the base model and training only two small matrices (A and B) for selected layers.\n",
    "This reduces training cost by 100‚Äì1000√ó.\n",
    "\n",
    "For a normal weight matrix $$ W \\in \\mathbb{R}^{d_out \\times d_in} $$, LoRA does:\n",
    "\n",
    "$$\n",
    "W' = W + \\Delta W\n",
    "$$\n",
    "\n",
    "But LoRA forces:\n",
    "\n",
    "$$\n",
    "\\Delta W = B A\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$\n",
    "A (Lower matrix) \\in \\mathbb{R}^{r \\times d_in}\n",
    "$$\n",
    "\n",
    "$$\n",
    "B (upper matrix) \\in \\mathbb{R}^{d_out \\times r}\n",
    "$$\n",
    "\n",
    "#### **Are A and B randomly initialized?**\n",
    "\n",
    "**A** and **B** are initialized as:\n",
    "\n",
    "- A: random (Gaussian)  \n",
    "- B: zeros\n",
    "\n",
    "This ensures:\n",
    "\n",
    "$$\n",
    "\\Delta W = B A = 0\n",
    "$$\n",
    "\n",
    "So at the beginning, LoRA does **not change the model at all**.\n",
    "\n",
    "The model slowly learns useful updates inside A and B.\n",
    "\n",
    "üüß **3. How LoRA Learns**\n",
    "\n",
    "During fine-tuning:\n",
    "\n",
    "- The base weight \\( W \\) stays **frozen**\n",
    "- Only \\( A \\) and \\( B \\) receive gradients\n",
    "- The rank \\( r \\) controls the learning capacity\n",
    "\n",
    "$$\n",
    "\\text{Larger } r \\;\\Rightarrow\\; \\text{learns more patterns}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Smaller } r \\;\\Rightarrow\\; \\text{cheaper and less risk of overfitting}\n",
    "$$\n",
    "\n",
    "###### When applied to GPT-3 175B, LoRA reduced trainable parameters by 10,000x and GPU memory requirements by 3x compared to full fine-tuning.\n",
    "\n",
    "üü¶ **6. When to Merge?**\n",
    "\n",
    "**Merge when you want:**\n",
    "- faster inference  \n",
    "- to export the model for deployment  \n",
    "- to avoid carrying separate adapter files  \n",
    "\n",
    "**Don‚Äôt merge if:**\n",
    "- you want to keep multiple adapters for different tasks  \n",
    "- you want composable LoRA modules (‚Äústacking‚Äù adapters)\n",
    "\n",
    "üü´ **7. Where LoRA Is Applied?**\n",
    "\n",
    "Typically applied to:\n",
    "\n",
    "- Attention **query/key/value** projections  \n",
    "- Sometimes **feed-forward** layers  \n",
    "- Not usually applied to **embeddings** or **layer norms**\n",
    "\n",
    "\n",
    "üü¶ **LoRA Hyperparameters: Œ± (alpha) and Dropout**\n",
    "\n",
    "LoRA has three important hyperparameters:\n",
    "\n",
    "- **rank (r)** ‚Äî size of A/B matrices  \n",
    "- **alpha (Œ±)** ‚Äî scaling factor  \n",
    "- **dropout** ‚Äî regularization to prevent overfitting  \n",
    "\n",
    "You already understand rank.  \n",
    "Now let‚Äôs break down **alpha** and **dropout**.\n",
    "\n",
    "---\n",
    "\n",
    "## üü© 1. What Is LoRA Alpha?\n",
    "\n",
    "LoRA alpha controls how strong the LoRA update is.\n",
    "\n",
    "LoRA produces:\n",
    "\n",
    "$$\n",
    "\\Delta W = BA\n",
    "$$\n",
    "\n",
    "But before adding it to the model, LoRA scales it:\n",
    "\n",
    "$$\n",
    "W' = W + \\frac{\\alpha}{r} (BA)\n",
    "$$\n",
    "\n",
    "So **alpha is a multiplier** on the LoRA update.\n",
    "\n",
    "### ‚úîÔ∏è Intuition for Alpha\n",
    "\n",
    "- Higher Œ± ‚Üí **stronger adaptation**  \n",
    "- Lower Œ± ‚Üí **softer adaptation**  \n",
    "\n",
    "Since LoRA uses a small rank (e.g., \\( r = 4 \\) or \\( r = 8 \\)),  \n",
    "**alpha compensates for the small size.**\n",
    "\n",
    "Typical values:\n",
    "\n",
    "- alpha = 16, 32, 64 \n",
    "\n",
    "**Rule of thumb:**\n",
    "\n",
    "- alpha approx 2 times r \n",
    "- alpha = 8 times r for harder tasks\n",
    "\n",
    "### üìå Why divide by r?\n",
    "\n",
    "To keep the update scale stable.\n",
    "\n",
    "- Larger \\( r \\) ‚Üí more parameters in \\( BA \\) ‚Üí update becomes stronger  \n",
    "- Dividing by \\( r \\) normalizes it\n",
    "\n",
    "This makes training **stable and consistent** regardless of rank.\n",
    "\n",
    "---\n",
    "\n",
    "## üüß 2. What Is LoRA Dropout?\n",
    "\n",
    "LoRA dropout randomly zeroes out **inputs to the LoRA adapter** during training.\n",
    "\n",
    "It does **not** affect:\n",
    "\n",
    "- the base model \\( W \\)  \n",
    "- inference-time behavior  \n",
    "\n",
    "It only affects training of \\( A \\) and \\( B \\), preventing overfitting.\n",
    "\n",
    "Example:\n",
    "\n",
    "- `dropout = 0.1` ‚Üí 10% of LoRA updates are dropped during training.\n",
    "\n",
    "### ‚úîÔ∏è Why Use LoRA Dropout?\n",
    "\n",
    "Because LoRA has **very few parameters**, it can overfit quickly, especially with:\n",
    "\n",
    "- small datasets  \n",
    "- easy tasks  \n",
    "- high learning rates  \n",
    "- large alpha  \n",
    "\n",
    "Dropout improves generalization.\n",
    "\n",
    "---\n",
    "\n",
    "## üü© Recommended Values\n",
    "\n",
    "| Setting | When to Use |\n",
    "|--------|--------------|\n",
    "| **dropout = 0.0** | Large dataset (100k+), stable training |\n",
    "| **dropout = 0.05‚Äì0.1** | Small datasets (<10k) |\n",
    "| **dropout = 0.15‚Äì0.3** | Very small datasets (<1k), strong regularization |\n",
    "\n",
    "---\n",
    "\n",
    "## üü™ Summary Table\n",
    "\n",
    "| Hyperparameter | Purpose | What happens when increased? |\n",
    "|----------------|---------|------------------------------|\n",
    "| **rank (r)** | adapter capacity | more expressive, more params |\n",
    "| **alpha (Œ±)** | strength of LoRA update | stronger effect on base model |\n",
    "| **dropout** | regularization | less overfitting, but slower learning |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867d1323",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46ade700",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
