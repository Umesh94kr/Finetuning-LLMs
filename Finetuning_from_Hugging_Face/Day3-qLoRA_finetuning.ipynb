{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac759252",
   "metadata": {},
   "source": [
    "# üß† Q-LoRA: Precision Used in Training & Inference\n",
    "\n",
    "This note explains:\n",
    "\n",
    "- Whether Q-LoRA uses **4-bit** weights during inference  \n",
    "- What precision LoRA adapters use  \n",
    "- Why adapters are kept in higher precision  \n",
    "- How inference is actually computed  \n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 1. Does Q-LoRA use the 4-bit model during inference?\n",
    "\n",
    "### ‚úîÔ∏è Yes ‚Äî the base model stays in **4-bit (NF4)** even during inference.\n",
    "\n",
    "Q-LoRA quantizes the *pretrained* model weights into 4-bit and **keeps them frozen**:\n",
    "\n",
    "W_base_4bit (frozen)\n",
    "\n",
    "\n",
    "These 4-bit weights are used both:\n",
    "\n",
    "- during training  \n",
    "- during inference  \n",
    "\n",
    "unless you explicitly merge the LoRA weights.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 2. Are LoRA adapters also 4-bit?\n",
    "\n",
    "### ‚ùå No ‚Äî LoRA adapters always stay in **higher precision** (FP16 or BF16).\n",
    "\n",
    "LoRA consists of two small matrices:\n",
    "\n",
    "- **A** (r √ó d)  \n",
    "- **B** (d √ó r)  \n",
    "\n",
    "These matrices must remain in **FP16/BF16** to avoid losing fine-grained update signals.\n",
    "\n",
    "### Summary:\n",
    "\n",
    "| Component | Precision | Why? |\n",
    "|----------|-----------|------|\n",
    "| Base model weights | **4-bit NF4** | Memory savings |\n",
    "| LoRA adapters (A, B) | **FP16/BF16** | Accuracy needed |\n",
    "| Gradients | FP16/BF16 | Required for training |\n",
    "\n",
    "---\n",
    "\n",
    "## üßÆ 3. How LoRA is applied to the 4-bit base model\n",
    "\n",
    "The LoRA update is:\n",
    "\n",
    "\\[\n",
    "\\Delta W = B A \\cdot \\frac{\\alpha}{r}\n",
    "\\]\n",
    "\n",
    "Then inference uses:\n",
    "\n",
    "\\[\n",
    "W_{\\text{effective}} = W_{4bit} + \\Delta W\n",
    "\\]\n",
    "\n",
    "This is applied **on the fly** at runtime.  \n",
    "The model stays quantized; only the adapter update is high precision.\n",
    "\n",
    "---\n",
    "\n",
    "## üîß 4. Two Inference Modes in Q-LoRA\n",
    "\n",
    "### **A) Default Q-LoRA Inference (recommended)**  \n",
    "- Base model: **4-bit**  \n",
    "- LoRA adapters: **FP16**  \n",
    "- Applied dynamically during forward pass\n",
    "\n",
    "Memory-efficient, fast, and most common.\n",
    "\n",
    "---\n",
    "\n",
    "### **B) Merged Inference (optional)**  \n",
    "You can merge:\n",
    "\n",
    "\\[\n",
    "W_{\\text{merged}} = W_{4bit} + \\Delta W\n",
    "\\]\n",
    "\n",
    "This produces a merged **FP16** model.\n",
    "\n",
    "- No adapters needed afterward  \n",
    "- Larger memory footprint  \n",
    "- Good for deployment\n",
    "\n",
    "---\n",
    "\n",
    "## üìä 5. Memory Summary\n",
    "\n",
    "| Stage | Base Model | LoRA | Total Memory |\n",
    "|-------|------------|------|--------------|\n",
    "| Training | 4-bit | FP16 | **Very Low (Q-LoRA advantage)** |\n",
    "| Inference (default) | 4-bit | FP16 | **Low** |\n",
    "| Inference (merged) | FP16 merged | ‚Äî | **Much higher** |\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Final Takeaways\n",
    "\n",
    "- ‚úîÔ∏è Base model stays **4-bit** during inference  \n",
    "- ‚ùå LoRA adapters are **not quantized** ‚Äî stay in FP16/BF16  \n",
    "- ‚úîÔ∏è Inference combines 4-bit base + FP16 LoRA adapters  \n",
    "- ‚úîÔ∏è Optionally, weights can be merged afterward  \n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you want a markdown section on:\n",
    "- How to load a Q-LoRA model and run inference in HF  \n",
    "- How double quantization works  \n",
    "- Why NF4 quantization improves training  \n",
    "- Q-LoRA memory calculations for 7B / 13B / 70B models\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
